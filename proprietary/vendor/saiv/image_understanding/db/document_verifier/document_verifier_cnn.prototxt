layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 128
      dim: 128
    }
  }
}
layer {
  name: "normalize"
  type: "Scale"
  bottom: "data"
  top: "scaled"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  scale_param {
    filler {
      value: 0.0078125
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "209"
  type: "Convolution"
  bottom: "scaled"
  top: "209"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "211"
  type: "ReLU"
  bottom: "209"
  top: "211"
}
layer {
  name: "212"
  type: "DepthwiseConvolution"
  bottom: "211"
  top: "212"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 16
    pad_h: 0
    pad_w: 0
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "214"
  type: "ReLU"
  bottom: "212"
  top: "214"
}
layer {
  name: "215"
  type: "Convolution"
  bottom: "214"
  top: "215"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "217"
  type: "Convolution"
  bottom: "215"
  top: "217"
  convolution_param {
    num_output: 72
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "219"
  type: "ReLU"
  bottom: "217"
  top: "219"
}
layer {
  name: "220"
  type: "DepthwiseConvolution"
  bottom: "219"
  top: "220"
  convolution_param {
    num_output: 72
    bias_term: true
    group: 72
    pad_h: 0
    pad_w: 0
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "222"
  type: "ReLU"
  bottom: "220"
  top: "222"
}
layer {
  name: "223"
  type: "Convolution"
  bottom: "222"
  top: "223"
  convolution_param {
    num_output: 24
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "225"
  type: "Convolution"
  bottom: "223"
  top: "225"
  convolution_param {
    num_output: 88
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "227"
  type: "ReLU"
  bottom: "225"
  top: "227"
}
layer {
  name: "228"
  type: "DepthwiseConvolution"
  bottom: "227"
  top: "228"
  convolution_param {
    num_output: 88
    bias_term: true
    group: 88
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "230"
  type: "ReLU"
  bottom: "228"
  top: "230"
}
layer {
  name: "231"
  type: "Convolution"
  bottom: "230"
  top: "231"
  convolution_param {
    num_output: 24
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "233"
  type: "Eltwise"
  bottom: "231"
  bottom: "223"
  top: "233"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "234"
  type: "Convolution"
  bottom: "233"
  top: "234"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "236"
  type: "ReLU"
  bottom: "234"
  top: "236"
}
layer {
  name: "237"
  type: "DepthwiseConvolution"
  bottom: "236"
  top: "237"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 96
    pad_h: 0
    pad_w: 0
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "239"
  type: "ReLU"
  bottom: "237"
  top: "239"
}
layer {
  name: "240"
  type: "Convolution"
  bottom: "239"
  top: "240"
  convolution_param {
    num_output: 40
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "242"
  type: "Convolution"
  bottom: "240"
  top: "242"
  convolution_param {
    num_output: 240
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "244"
  type: "ReLU"
  bottom: "242"
  top: "244"
}
layer {
  name: "245"
  type: "DepthwiseConvolution"
  bottom: "244"
  top: "245"
  convolution_param {
    num_output: 240
    bias_term: true
    group: 240
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "247"
  type: "ReLU"
  bottom: "245"
  top: "247"
}
layer {
  name: "248"
  type: "Convolution"
  bottom: "247"
  top: "248"
  convolution_param {
    num_output: 40
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "250"
  type: "Eltwise"
  bottom: "248"
  bottom: "240"
  top: "250"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "251"
  type: "Convolution"
  bottom: "250"
  top: "251"
  convolution_param {
    num_output: 240
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "253"
  type: "ReLU"
  bottom: "251"
  top: "253"
}
layer {
  name: "254"
  type: "DepthwiseConvolution"
  bottom: "253"
  top: "254"
  convolution_param {
    num_output: 240
    bias_term: true
    group: 240
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "256"
  type: "ReLU"
  bottom: "254"
  top: "256"
}
layer {
  name: "257"
  type: "Convolution"
  bottom: "256"
  top: "257"
  convolution_param {
    num_output: 40
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "259"
  type: "Eltwise"
  bottom: "257"
  bottom: "250"
  top: "259"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "260"
  type: "Convolution"
  bottom: "259"
  top: "260"
  convolution_param {
    num_output: 120
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "262"
  type: "ReLU"
  bottom: "260"
  top: "262"
}
layer {
  name: "263"
  type: "DepthwiseConvolution"
  bottom: "262"
  top: "263"
  convolution_param {
    num_output: 120
    bias_term: true
    group: 120
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "265"
  type: "ReLU"
  bottom: "263"
  top: "265"
}
layer {
  name: "266"
  type: "Convolution"
  bottom: "265"
  top: "266"
  convolution_param {
    num_output: 48
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "268"
  type: "Convolution"
  bottom: "266"
  top: "268"
  convolution_param {
    num_output: 144
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "270"
  type: "ReLU"
  bottom: "268"
  top: "270"
}
layer {
  name: "271"
  type: "DepthwiseConvolution"
  bottom: "270"
  top: "271"
  convolution_param {
    num_output: 144
    bias_term: true
    group: 144
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "273"
  type: "ReLU"
  bottom: "271"
  top: "273"
}
layer {
  name: "274"
  type: "Convolution"
  bottom: "273"
  top: "274"
  convolution_param {
    num_output: 48
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "276"
  type: "Eltwise"
  bottom: "274"
  bottom: "266"
  top: "276"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "277"
  type: "Convolution"
  bottom: "276"
  top: "277"
  convolution_param {
    num_output: 288
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "279"
  type: "ReLU"
  bottom: "277"
  top: "279"
}
layer {
  name: "280"
  type: "DepthwiseConvolution"
  bottom: "279"
  top: "280"
  convolution_param {
    num_output: 288
    bias_term: true
    group: 288
    pad_h: 0
    pad_w: 0
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "282"
  type: "ReLU"
  bottom: "280"
  top: "282"
}
layer {
  name: "283"
  type: "Convolution"
  bottom: "282"
  top: "283"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "285"
  type: "Convolution"
  bottom: "283"
  top: "285"
  convolution_param {
    num_output: 576
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "287"
  type: "ReLU"
  bottom: "285"
  top: "287"
}
layer {
  name: "288"
  type: "DepthwiseConvolution"
  bottom: "287"
  top: "288"
  convolution_param {
    num_output: 576
    bias_term: true
    group: 576
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "290"
  type: "ReLU"
  bottom: "288"
  top: "290"
}
layer {
  name: "291"
  type: "Convolution"
  bottom: "290"
  top: "291"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "293"
  type: "Eltwise"
  bottom: "291"
  bottom: "283"
  top: "293"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "294"
  type: "Convolution"
  bottom: "293"
  top: "294"
  convolution_param {
    num_output: 576
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "296"
  type: "ReLU"
  bottom: "294"
  top: "296"
}
layer {
  name: "297"
  type: "DepthwiseConvolution"
  bottom: "296"
  top: "297"
  convolution_param {
    num_output: 576
    bias_term: true
    group: 576
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "299"
  type: "ReLU"
  bottom: "297"
  top: "299"
}
layer {
  name: "300"
  type: "Convolution"
  bottom: "299"
  top: "300"
  convolution_param {
    num_output: 96
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "302"
  type: "Eltwise"
  bottom: "300"
  bottom: "293"
  top: "302"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "303"
  type: "Convolution"
  bottom: "302"
  top: "303"
  convolution_param {
    num_output: 576
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "305"
  type: "ReLU"
  bottom: "303"
  top: "305"
}
layer {
  name: "306"
  type: "Pooling"
  bottom: "305"
  top: "306"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "307"
  type: "Convolution"
  bottom: "306"
  top: "307"
  convolution_param {
    num_output: 1024
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "308"
  type: "ReLU"
  bottom: "307"
  top: "308"
}
layer {
  name: "moon/fc"
  type: "InnerProduct"
  bottom: "308"
  top: "moon/fc"
  inner_product_param {
    num_output: 2
    bias_term: true
  }
}
layer {
  name: "prob_baseline"
  type: "Softmax"
  bottom: "moon/fc"
  top: "prob_baseline"
}
